{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEWARE** This notebook is for made available to the public for reproducability purposes only. The code below is not maintained. \n",
    "\n",
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:10.486317Z",
     "start_time": "2020-06-23T13:54:09.109870Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import SVG, display, Image\n",
    "from os.path import dirname\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sklearn.manifold import *\n",
    "import os\n",
    "import glob\n",
    "import mdtraj as md\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt; \n",
    "plt.style.use('seaborn-colorblind')\n",
    "from functools import reduce\n",
    "\n",
    "# Make sure to add state sampling and demystifying to your python path, e.g\n",
    "sys.path.append(\"/home/oliverfl/git/delemottelab/demystifying\")\n",
    "sys.path.append(dirname(\"../../../state_sampling\"))\n",
    "import demystifying as dm\n",
    "from statesampling import log, colvars, utils\n",
    "\n",
    "\n",
    "_log =  log.getLogger(\"analysis\")\n",
    "_log.setLevel('DEBUG')\n",
    "\n",
    "ligands = np.array([\n",
    "    'apo', \n",
    "    'carazolol',      \n",
    "    'alprenolol', \n",
    "    'timolol',  \n",
    "    'salmeterol',      \n",
    "    'adrenaline',      \n",
    "    'p0g'\n",
    "])\n",
    "colors = np.array(['darkkhaki', \n",
    "                   'olive', \n",
    "                   'forestgreen', \n",
    "                   'chartreuse', \n",
    "                   'darkslategray',\n",
    "                   'slateblue', \n",
    "                   'midnightblue', \n",
    "                   'silver', \n",
    "                   'pink', \n",
    "                   'darksalmon',\n",
    "                  ])\n",
    "markers = dict(\n",
    "    apo=\"o\", \n",
    "    carazolol=\">\",      \n",
    "    alprenolol=\"^\", \n",
    "    timolol=\"<\",  \n",
    "    salmeterol=\"s\",      \n",
    "    adrenaline=\"p\",      \n",
    "    p0g=\"h\"\n",
    ")\n",
    "\n",
    "trajs=[] #Optionally you can load MDTraj trajectories into this list.\n",
    "working_dir = \"../.simu/\"\n",
    "topology = md.load(working_dir + \"apo-equilibrated.gro\").topology\n",
    "traj_type=\"strings\" #choose between 'strings' and 'single_state_sampling'\n",
    "feature_type=\"inv__contacts__closest-heavy\" \n",
    "group_by_type=False\n",
    "_log.info(\"Done. Using traj_type %s\", traj_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ligand Signaling Effects\n",
    "## Define values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:10.665451Z",
     "start_time": "2020-06-23T13:54:10.487410Z"
    }
   },
   "outputs": [],
   "source": [
    "ligand_to_type = {\n",
    "    'carazolol' : 'not agonist',\n",
    "    'apo' :  'not agonist',\n",
    "    'adrenaline' : 'agonist',\n",
    "    'alprenolol': 'not agonist',\n",
    "    'p0g' : 'agonist',\n",
    "    'salmeterol': 'agonist',\n",
    "    'timolol': 'not agonist'\n",
    "}\n",
    "ligand_types = [\n",
    "    'not agonist',\n",
    "    'agonist'\n",
    "]\n",
    "ligand_to_abbreviation = {\n",
    "    'carazolol' : 'CAU',\n",
    "    'apo' :  'APO',\n",
    "    'adrenaline' : 'ALE',\n",
    "    'alprenolol': 'ALP',\n",
    "    'p0g' : 'P0G',\n",
    "    'salmeterol': 'SAL',\n",
    "    'timolol': 'TIM', \n",
    "}\n",
    "ligand_abbreviations = np.array([ligand_to_abbreviation[l] for l in ligands])    \n",
    "ligand_to_effect = {\n",
    "    #See http://molpharm.aspetjournals.org/content/85/3/492/tab-figures-data\n",
    "    #The lower the EC50, the less the concentration of a drug is required to produce 50% of maximum effect and the higher the potency\n",
    "    #measured as pEC50 (high values -> strong response )\n",
    "    'apo' :  dict(\n",
    "        cAMP_pEC50=0, \n",
    "        cAMP_Emax=0.,\n",
    "        cAMP_Emax_ste=None,\n",
    "        pERK12_Emax=0,\n",
    "        Ca2_Emax=0,\n",
    "        Endocytosis_Emax=0,\n",
    "        exval_tm5_bulge=1.24309369,\n",
    "        exval_Connector_deltaRMSD=0.02319871, \n",
    "        exval_TM6_TM3_distance=0.86821019,\n",
    "        exval_Ionic_lock_distance=0.98442891,\n",
    "        exval_YY_motif=1.23168429,        \n",
    "        exval_Pro211_Phe282=0.61553677,  \n",
    "        exval_tm5_bulge_ste=0.00020138,\n",
    "        exval_Connector_deltaRMSD_ste=0.00011046, \n",
    "        exval_TM6_TM3_distance_ste=0.00146633,\n",
    "        exval_Ionic_lock_distance_ste=0.0023686,\n",
    "        exval_YY_motif_ste=0.00167255,        \n",
    "        exval_Pro211_Phe282_ste=None,   \n",
    "        awh_nb_tm5=1.2993,\n",
    "        exval_TM6_TM3_distance_ca=1.05442884,\n",
    "        exval_TM6_TM3_distance_ca_ste=None,                \n",
    "    ),    \n",
    "    'carazolol' : dict(\n",
    "        cAMP_pEC50=0, \n",
    "        cAMP_Emax=0.,\n",
    "        cAMP_Emax_ste=None,        \n",
    "        pERK12_Emax=0,\n",
    "        Ca2_Emax=0,\n",
    "        Endocytosis_Emax=0,\n",
    "        exval_tm5_bulge=1.22956958,\n",
    "        exval_Connector_deltaRMSD=0.017588282, \n",
    "        exval_TM6_TM3_distance=0.81141607,\n",
    "        exval_Ionic_lock_distance=0.92297367,\n",
    "        exval_YY_motif=1.1612276,        \n",
    "        exval_Pro211_Phe282=0.59886003,  \n",
    "        exval_tm5_bulge_ste=0.00036111,\n",
    "        exval_Connector_deltaRMSD_ste=8.41365600e-05, \n",
    "        exval_TM6_TM3_distance_ste=0.00203712,\n",
    "        exval_Ionic_lock_distance_ste=0.00321631,\n",
    "        exval_YY_motif_ste=0.00243157,        \n",
    "        exval_Pro211_Phe282_ste=None,   \n",
    "        awh_nb_tm5=1.3218,    \n",
    "        exval_TM6_TM3_distance_ca=1.105435,\n",
    "        exval_TM6_TM3_distance_ca_ste=None,  \n",
    "        \n",
    "    ), # carazolol's experimental taken from apo. cannot use carvedilol since it is a G protein antagonist and arrest agonist...\n",
    "    'alprenolol':  dict(\n",
    "        cAMP_pEC50=9.81, \n",
    "        cAMP_Emax=35.76,\n",
    "        cAMP_Emax_ste=3.70,        \n",
    "        pERK12_Emax=102.3,\n",
    "        Ca2_Emax=0,\n",
    "        Endocytosis_Emax=0,      \n",
    "        exval_tm5_bulge=1.21801455,\n",
    "        exval_Connector_deltaRMSD=0.02275429, \n",
    "        exval_TM6_TM3_distance=0.75567965,\n",
    "        exval_Ionic_lock_distance=0.94340876,\n",
    "        exval_YY_motif=1.23447094,           \n",
    "        exval_Pro211_Phe282=0.57185033,   \n",
    "        exval_tm5_bulge_ste=0.0001231,\n",
    "        exval_Connector_deltaRMSD_ste=0.00014594, \n",
    "        exval_TM6_TM3_distance_ste=0.00232501,\n",
    "        exval_Ionic_lock_distance_ste=0.00246458,\n",
    "        exval_YY_motif_ste=0.00228454,        \n",
    "        exval_Pro211_Phe282_ste=None,   \n",
    "        awh_nb_tm5=1.3143,    \n",
    "        exval_TM6_TM3_distance_ca=1.05058275,\n",
    "        exval_TM6_TM3_distance_ca_ste=None,       \n",
    "    ), \n",
    "    'timolol': dict(\n",
    "        cAMP_pEC50=8.81, \n",
    "        cAMP_Emax=-44.45,\n",
    "        cAMP_Emax_ste=5.28,        \n",
    "        pERK12_Emax=0,\n",
    "        Ca2_Emax=0,\n",
    "        Endocytosis_Emax=0,\n",
    "        exval_tm5_bulge=1.28462016,\n",
    "        exval_Connector_deltaRMSD=0.01608262, \n",
    "        exval_TM6_TM3_distance=0.83716437,\n",
    "        exval_Ionic_lock_distance=0.95976886,\n",
    "        exval_YY_motif=1.42438547,          \n",
    "        exval_Pro211_Phe282=0.61874432,   \n",
    "        exval_tm5_bulge_ste=0.00030679,\n",
    "        exval_Connector_deltaRMSD_ste=0.00021453, \n",
    "        exval_TM6_TM3_distance_ste=0.0033251,\n",
    "        exval_Ionic_lock_distance_ste=0.00569959,\n",
    "        exval_YY_motif_ste=0.00087478,        \n",
    "        exval_Pro211_Phe282_ste=None,   \n",
    "        awh_nb_tm5=1.347,       \n",
    "        exval_TM6_TM3_distance_ca=1.0556941,\n",
    "        exval_TM6_TM3_distance_ca_ste=None,     \n",
    "    ),     \n",
    "    'salmeterol': dict(\n",
    "        cAMP_pEC50=8.63, \n",
    "        cAMP_Emax=105.5,\n",
    "        cAMP_Emax_ste=6.11, \n",
    "        pERK12_Emax=74.39,\n",
    "        Ca2_Emax=34.45,\n",
    "        Endocytosis_Emax=0,\n",
    "        exval_tm5_bulge=1.14754131,\n",
    "        exval_Connector_deltaRMSD=0.04149619, \n",
    "        exval_TM6_TM3_distance=0.85799139,\n",
    "        exval_Ionic_lock_distance=1.00344138,\n",
    "        exval_YY_motif=1.14292594,          \n",
    "        exval_Pro211_Phe282=0.57731706,   \n",
    "        exval_tm5_bulge_ste=0.00016367,\n",
    "        exval_Connector_deltaRMSD_ste=4.00103042e-05, \n",
    "        exval_TM6_TM3_distance_ste=0.00184956,\n",
    "        exval_Ionic_lock_distance_ste=0.00448551,\n",
    "        exval_YY_motif_ste=0.00067195,        \n",
    "        exval_Pro211_Phe282_ste=None,    \n",
    "        awh_nb_tm5=1.2028,  \n",
    "        exval_TM6_TM3_distance_ca=1.06233784,\n",
    "        exval_TM6_TM3_distance_ca_ste=None,          \n",
    "    ),\n",
    "    'adrenaline' : dict(\n",
    "        cAMP_pEC50=7.78, \n",
    "        cAMP_Emax=74.04,\n",
    "        cAMP_Emax_ste=6.19, \n",
    "        pERK12_Emax=157.7,\n",
    "        Ca2_Emax=112.0,\n",
    "        Endocytosis_Emax=115.5,  \n",
    "        exval_tm5_bulge=1.12983003,\n",
    "        exval_Connector_deltaRMSD=0.02935141, \n",
    "        exval_TM6_TM3_distance=0.8329062,\n",
    "        exval_Ionic_lock_distance=0.96943942,\n",
    "        exval_YY_motif=0.86182756,            \n",
    "        exval_Pro211_Phe282=0.58856012,    \n",
    "        exval_tm5_bulge_ste=0.00017935,\n",
    "        exval_Connector_deltaRMSD_ste=4.66225665e-05, \n",
    "        exval_TM6_TM3_distance_ste=0.00114867,\n",
    "        exval_Ionic_lock_distance_ste=0.00524182,\n",
    "        exval_YY_motif_ste=0.000854,        \n",
    "        exval_Pro211_Phe282_ste=None,   \n",
    "        awh_nb_tm5=1.237, \n",
    "        exval_TM6_TM3_distance_ca=1.08249786,\n",
    "        exval_TM6_TM3_distance_ca_ste=None,     \n",
    "    ),    \n",
    "    'p0g' : dict(\n",
    "        # experimental values taken from isoprotenerol.\n",
    "        cAMP_pEC50=8.23, \n",
    "        cAMP_Emax=100.,\n",
    "        cAMP_Emax_ste=0.15, \n",
    "        pERK12_Emax=100.,\n",
    "        Ca2_Emax=100.,\n",
    "        Endocytosis_Emax=100.,\n",
    "        exval_tm5_bulge=1.11438486,\n",
    "        exval_Connector_deltaRMSD=0.04348406, \n",
    "        exval_TM6_TM3_distance=0.78827263,\n",
    "        exval_Ionic_lock_distance=0.95074948,\n",
    "        exval_YY_motif=1.22048298,              \n",
    "        exval_Pro211_Phe282=0.57743262,    \n",
    "        exval_tm5_bulge_ste=0.00010445,\n",
    "        exval_Connector_deltaRMSD_ste=3.66579016e-05, \n",
    "        exval_TM6_TM3_distance_ste=0.00057464,\n",
    "        exval_Ionic_lock_distance_ste=0.00098435,\n",
    "        exval_YY_motif_ste=0.00270393,        \n",
    "        exval_Pro211_Phe282_ste=None,   \n",
    "        awh_nb_tm5=1.2462,      \n",
    "        exval_TM6_TM3_distance_ca=1.03998389,\n",
    "        exval_TM6_TM3_distance_ca_ste=None,  \n",
    "    ),     \n",
    "}\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:10.952853Z",
     "start_time": "2020-06-23T13:54:10.666700Z"
    }
   },
   "outputs": [],
   "source": [
    "def _fix_label(l):\n",
    "    if l.startswith(\"exval_\"):\n",
    "        l = \"E[{}] [nm]\".format(l.replace(\"exval_\", \"\"))\n",
    "    l = l.replace(\"_\", \" \")\n",
    "    l = l.replace(\"tm\", \"TM\")\n",
    "    l = l.replace(\"delta delta\", \"deltadelta\")\n",
    "    l = l.replace(\"delta\", \"$\\Delta$\")\n",
    "    l = l.replace(\"connector\", \"Connector\")\n",
    "    l = l.replace(\"p0g\", \"BI-167107\")\n",
    "    l = l.replace(\"YY\", \"Y-Y\")\n",
    "    l = l.replace(\"Emax\", \"Emax [%]\")\n",
    "    return l\n",
    "\n",
    "def correlation_plot(xdim='tm5_basin', ydim='cAMP_Emax', \n",
    "                     ligands=ligands, \n",
    "                     ligand_to_effect=ligand_to_effect,\n",
    "                     prefix='', \n",
    "                     predict=False):\n",
    "    fig = plt.figure(figsize=(4,4))    \n",
    "    xvals = []\n",
    "    yvals = []\n",
    "    to_predict = []\n",
    "    def add_to_graph(l_idx, x, y, prediction=False, xerr=None, yerr=None):\n",
    "        xvals.append(x)\n",
    "        yvals.append(y)\n",
    "        color=colors[l_idx]\n",
    "        plt.errorbar(x,y, fmt='o', color=color, xerr=xerr, yerr=yerr)\n",
    "        txt=\"  \" + _fix_label(ligands[l_idx]).capitalize() + (\"*\" if prediction else \"\")\n",
    "        plt.text(x,y, txt)\n",
    "        \n",
    "    for l_idx, l in enumerate(ligands):\n",
    "        vals = ligand_to_effect.get(l, None)\n",
    "        if vals is None:\n",
    "            continue\n",
    "        x = vals.get(xdim, None)\n",
    "        y = vals.get(ydim, None)\n",
    "        if x is None or (y is None and not predict):\n",
    "            continue\n",
    "        elif y is None:\n",
    "            to_predict.append((l_idx, x))\n",
    "            continue\n",
    "        add_to_graph(l_idx, x, y, xerr=vals.get(xdim + \"_ste\", None), yerr=vals.get(ydim + \"_ste\", None))\n",
    "    if len(xvals) is None:\n",
    "        _log.warn(\"No input found\")\n",
    "        return\n",
    "    \n",
    "    #Regression\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(xvals, yvals)\n",
    "    \n",
    "    #Make predictions\n",
    "    for (l_idx, x) in to_predict:\n",
    "        add_to_graph(l_idx, x, x*slope + intercept, prediction=True)\n",
    "        \n",
    "    #Visualize\n",
    "    xvals = np.array(xvals)\n",
    "    xlin = np.linspace(xvals.min()  - xvals.std()/4, xvals.max() + xvals.std()/4, 10)\n",
    "    plt.plot(xlin, xlin*slope + intercept, linestyle='--', color=\"grey\", alpha=0.3, linewidth=5)\n",
    "    plt.ylabel(_fix_label(ydim))\n",
    "    plt.xlabel(_fix_label(xdim))\n",
    "    plt.xlim([xlin.min(), xlin.max()])\n",
    "    plt.title(\"R = {:.2f}, p = {:.3f}\".format(r_value, p_value))   \n",
    "    plt.tight_layout(pad=0.3)   \n",
    "    if not os.path.exists(\"output/correlations\"):\n",
    "        os.makedirs(\"output/correlations\")\n",
    "    plt.savefig(\"output/correlations/{}{}_{}.svg\".format(prefix, xdim, ydim)) \n",
    "    plt.show()\n",
    "        \n",
    "correlation_plot(\n",
    "    xdim='exval_tm5_bulge',   #exval_tm5_bulge, exval_Connector_deltaRMSD, exval_YY_motif\n",
    "    #exval_TM6_TM3_distance, exval_Ionic_lock_distance, exval_Pro211_Phe282\n",
    "    ydim='cAMP_Emax', #cAMP_Emax, Endocytosis_Emax, pERK12_Emax, Ca2_Emax\n",
    "    predict=True,\n",
    ")\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy arry IO\n",
    "**NOTE** that the dataset files need to be downloaded from https://drive.google.com/drive/folders/16_9JX3z2Vmly4ZdNTTG_WqiByzS5mNfc?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:10.957597Z",
     "start_time": "2020-06-23T13:54:10.954169Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir=\"output/datasets/{}/{}/\".format(traj_type, feature_type)\n",
    "_log.info(\"Using dataset_dir %s\", dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:14.763372Z",
     "start_time": "2020-06-23T13:54:10.959477Z"
    }
   },
   "outputs": [],
   "source": [
    "with np.load(dataset_dir + \"data.npz\", allow_pickle=True) as data:\n",
    "    samples=data['samples'] \n",
    "    labels=data['labels']\n",
    "    ligand_labels=data['ligand_labels']\n",
    "    feature_to_resids=data['feature_to_resids']\n",
    "    scaler=data['scaler']\n",
    "    # Little hack to get the scaler out of the object array below\n",
    "    scaler.shape=(1,)\n",
    "    scaler = scaler[0]\n",
    "if labels.shape[1] != len(ligands):\n",
    "    raise Exception(\"Number of loaded classes {} differs from the number of defined ligands {}\".format(labels.shape[1], len(ligands)))\n",
    "_log.info(\"Done. Loaded samples of shape %s and labels for %s classes\", samples.shape, labels.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:41.602042Z",
     "start_time": "2020-06-23T13:54:14.764621Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "if samples is not None:\n",
    "    np.savez_compressed(dataset_dir + \"data\", \n",
    "                    samples=samples, \n",
    "                    labels=labels, \n",
    "                    ligand_labels=ligand_labels, \n",
    "                    feature_to_resids=feature_to_resids,\n",
    "                    scaler=scaler\n",
    "                   )\n",
    "_log.info(\"Done. Saved to %s\", dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.550432Z",
     "start_time": "2020-06-23T13:54:41.603577Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "residue_mapping = pd.read_csv(working_dir + \"beta2_generic_residues.csv\", \n",
    "                              delimiter=';',\n",
    "                             dtype={'generic':'object', 'beta2':'int'})\n",
    "\n",
    "def fix_generic_numbers(residues):\n",
    "    for r in residues:\n",
    "        if not r.is_protein:\n",
    "            continue\n",
    "        generic = residue_mapping[residue_mapping['beta2'] == r.resSeq]['generic']\n",
    "        generic = generic.values[0] if len(generic) > 0 else None\n",
    "        r.generic= generic        \n",
    "        if generic is None:\n",
    "            r.fullname = \"{}{}\".format(r.code, r.resSeq)\n",
    "        else:\n",
    "            r.fullname = \"{}{}({})\".format(r.code, r.resSeq, r.generic)\n",
    "\n",
    "def find_residue(res_id, topology=topology):\n",
    "    q = \"protein and resSeq {}\".format(res_id)\n",
    "    return topology.atom(topology.select(q)[0]).residue\n",
    "            \n",
    "def create_index_to_residue(feature_to_resids, topology=topology):\n",
    "    res = []\n",
    "    seen_residues = set()\n",
    "    for axis in range(feature_to_resids.shape[1]):\n",
    "        for fr in feature_to_resids[:,axis]:\n",
    "            if fr in seen_residues:\n",
    "                continue\n",
    "            r = find_residue(fr, topology)\n",
    "            #generic = residue_mapping[residue_mapping['beta2'] == r.resSeq]['generic']\n",
    "            #generic = generic.values[0] if len(generic) > 0 else None\n",
    "            #r.generic= generic\n",
    "            #if generic is None:\n",
    "            #    r.fullname = \"{}{}\".format(r.code, r.resSeq)\n",
    "            #else:\n",
    "            #    r.fullname = \"{}{}({})\".format(r.code, r.resSeq, r.generic)\n",
    "            res.append(r)\n",
    "            seen_residues.add(fr)\n",
    "        \n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "def to_relevant_residues(topology=topology, \n",
    "                         ignored_residues = [],\n",
    "                         included_residues=None):\n",
    "    ignored_residues.append(24) #For some reasone this residue gives really huge rmsd for a few frames, maybe broken PBCs. \n",
    "    residues = [\n",
    "        r for r in topology.residues\n",
    "        if r.is_protein and r.resSeq not in ignored_residues\n",
    "    ]\n",
    "    if included_residues is not None and len(included_residues) > 0:\n",
    "        residues = [r for r in residues if r.resSeq in included_residues]        \n",
    "    return np.array(residues)\n",
    "\n",
    "\n",
    "def to_rmsd_cvs(topology=topology, ignored_residues=[], included_residues=None):\n",
    "    cvs = []\n",
    "    feature_to_resids = []\n",
    "    for r in to_relevant_residues(topology, ignored_residues=ignored_residues, included_residues=included_residues):\n",
    "        q = \"protein and resSeq {} and element != 'H'\".format(r.resSeq)\n",
    "        active_cv = colvars.cvs.RmsdCv(ID=\"active-rmsd_{}\".format(r), \n",
    "                                   name=\"Active RMSD {}\".format(r.fullname), \n",
    "                                   reference_structure=active_traj,\n",
    "                                   query=q)\n",
    "        inactive_cv = colvars.cvs.RmsdCv(ID=\"inactive-rmsd_{}\".format(r), \n",
    "                                   name=\"Inactive RMSD {}\".format(r.fullname), \n",
    "                                   reference_structure=inactive_traj,\n",
    "                                   query=q)\n",
    "        cvs.append(active_cv),\n",
    "        feature_to_resids.append([r.resSeq])        \n",
    "        cvs.append(inactive_cv)  \n",
    "        feature_to_resids.append([r.resSeq])\n",
    "    return np.array(cvs), np.array(feature_to_resids)\n",
    "\n",
    "\n",
    "def to_contact_cvs(topology=topology, scheme=\"ca\", inverse=True, ignored_residues=[], included_residues=None):\n",
    "    cvclass = colvars.InverseContactCv if inverse else colvars.ContactCv\n",
    "    residues = to_relevant_residues(topology, ignored_residues=ignored_residues, included_residues=included_residues)\n",
    "    residue_combos = []\n",
    "    for idx, r1 in enumerate(residues):\n",
    "        for r2 in residues[idx+3:]:\n",
    "            residue_combos.append((r1,r2))\n",
    "    cvs = np.array([\n",
    "        cvclass(res1=res1.resSeq, \n",
    "                res2=res2.resSeq, \n",
    "                scheme=scheme,\n",
    "                name=\"{}-{}\".format(res1.fullname, res2.fullname), \n",
    "                ID=\"|{}-{}|^{}({})\".format(res1, res2, -1 if inverse else 1, scheme))\n",
    "        for (res1, res2) in residue_combos\n",
    "    ])\n",
    "    feature_to_resids = np.array([[cv.res1, cv.res2] for cv in cvs])\n",
    "    return cvs, feature_to_resids\n",
    "\n",
    "def to_features(trajs, feature_type):\n",
    "    ignored_residues = caps_residues\n",
    "    included_residues = []\n",
    "    if \"noligand\" in feature_type:\n",
    "        ignored_residues += ligand_interactions\n",
    "        _log.info(\"Excluding ligand binding site interactions\")\n",
    "    if \"npxxy\" in feature_type:\n",
    "        included_residues += [322, 323, 324, 325, 326, 327]\n",
    "    if \"conserved\" in feature_type:\n",
    "        included_residues += [51, 79, 131, 158, 211, 288, 323]\n",
    "    if 'demystifying-cvs' in feature_type:\n",
    "        cvs = demystifying_cvs\n",
    "        feature_to_resids = np.array([[cv.res1, cv.res2] for cv in cvs])        \n",
    "    elif 'sidechain-rmsd' in feature_type:\n",
    "        cvs, feature_to_resids = to_rmsd_cvs(topology, ignored_residues=ignored_residues, included_residues=included_residues)\n",
    "    elif 'contacts__' in feature_type:\n",
    "        scheme = feature_type.split(\"__\")[-1]\n",
    "        inverse = 'inv__' in feature_type      \n",
    "        cvs, feature_to_resids = to_contact_cvs(topology, inverse=inverse, scheme=scheme, ignored_residues=ignored_residues, included_residues=included_residues)\n",
    "    else:\n",
    "        raise Exception(\"Invalid feature type {}\".format(feature_type))\n",
    "    for cv in cvs:\n",
    "        if cv.name is None:\n",
    "            cv.name = cv.id\n",
    "    return [colvars.eval_cvs(cvs, t) for t in trajs], cvs, feature_to_resids\n",
    "\n",
    "def fix_residue_format_on_cv_names(cvs,topology=topology):\n",
    "    for cv in cvs:\n",
    "        if hasattr(cv, \"res1\") and hasattr(cv, \"res2\"):\n",
    "            r1 = find_residue(cv.res1, topology)\n",
    "            r2 = find_residue(cv.res2, topology)\n",
    "            cv.name = \"{}-{}\".format(r1.fullname, r2.fullname)  \n",
    "\n",
    "fix_generic_numbers(topology.residues)\n",
    "caps_residues=[23, 27, 227, 266, 344]\n",
    "ligand_interactions = [109, 113, 114, 117, 193, 195, 203, 204, 207, 286, 289, 290, 293, 308, 309, 312]\n",
    "\n",
    "demystifying_cvs = colvars.io.load_cvs(working_dir + \"/cvs.json\")\n",
    "fix_residue_format_on_cv_names(demystifying_cvs)\n",
    "rmsd_cvs = to_rmsd_cvs(topology) if len(trajs) > 0 else []\n",
    "_log.info(\"Loaded %s demystifying cvs and %s rmsd cvs\", len(demystifying_cvs), len(rmsd_cvs))\n",
    "features, cvs, feature_to_resids = to_features(trajs, feature_type)\n",
    "index_to_residue = create_index_to_residue(feature_to_resids)\n",
    "if len(trajs) > 0:\n",
    "    _log.info(\"Computed %s features for %s datasets (%s)\", features[0].shape[1] , len(features), feature_type)\n",
    "else:\n",
    "    _log.info(\"Loaded nothing else since there are no trajectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.667620Z",
     "start_time": "2020-06-23T13:54:09.156Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_important_residues(importance, index_to_residue=index_to_residue, importance_cutoff=0.5, \n",
    "                            count_cutoff=100):\n",
    "    index_value_importance = [\n",
    "        (idx, imp) \n",
    "        for (idx,imp) in \n",
    "        sorted(enumerate(importance), key=lambda t: t[1], reverse=True)\n",
    "    ]\n",
    "    res = dict()\n",
    "    for (idx, imp) in index_value_importance:\n",
    "        if imp < importance_cutoff or len(res) == count_cutoff:\n",
    "            break\n",
    "        residue = index_to_residue[idx]\n",
    "        label = residue.fullname\n",
    "        res[label] = residue.resSeq\n",
    "    default = _get_default_important_residues()\n",
    "    return dict(**res, **default)\n",
    "\n",
    "def _get_default_important_residues(supervised=None, feature_type=feature_type):\n",
    "    # From 2020 BPJ paper \n",
    "    res = dict(\n",
    "        #npxxy = [322, 323, 324, 325, 326],\n",
    "        #yy = [219, 326],\n",
    "        #ligand_interactions = [109, 113, 114, 117, 193, 195, 203, 204, 207, 286, 289, 290, 293, 308, 309, 312],\n",
    "        most_conserved_TM_residues = [51, 79, 131, 158, 211, 288, 323, 332],\n",
    "        #dry = [130, 131, 132],\n",
    "        #pif = [121, 211, 282],\n",
    "        #m82 = [82],\n",
    "        g_prot_interactions=[131, 134, 135, 136, 138, 139, 141, 142, 143, \n",
    "        222, 225, 226, 228, 229, 230, 232, 233, 271, 274, 275],\n",
    "\n",
    "    )\n",
    "    \"\"\" OLD rom original paper\n",
    "    if supervised:\n",
    "        if \"rmsd\" in feature_type:\n",
    "            return {\n",
    "                # 'Ligand interactions': ligand_interactions,\n",
    "                'PIF': pif,\n",
    "                'M82': [82],  # , 286, 316],\n",
    "                'DRY': dry,\n",
    "                # 'NPxxY': npxxy,\n",
    "                # 'Most conserved TM residues': most_conserved_TM_residues\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                #'Ligand interactions': ligand_interactions,\n",
    "                #'D79': [79],\n",
    "                'E268': [268],\n",
    "                #'L144': [144],\n",
    "                'NPxxY': npxxy,\n",
    "                \n",
    "            }\n",
    "    else:\n",
    "        return {\n",
    "            'NPxxY': npxxy,\n",
    "            'End of TM6': [268, 272, 275, 279],\n",
    "            'L144': [144],\n",
    "        }\n",
    "    \"\"\"\n",
    "    return res\n",
    "\n",
    "\n",
    "demystifying_dir = \"output/demystifying/\"\n",
    "def extract_features(feature_extractors, overwrite=False):\n",
    "    results_dir = \"{wdir}/{traj_type}/{feature_type}/{by_type}/\".format(\n",
    "        wdir=demystifying_dir,\n",
    "        traj_type=traj_type,\n",
    "        feature_type=feature_type,\n",
    "        by_type=\"by_type\" if group_by_type else \"\"\n",
    "    )   \n",
    "    postprocessors = []\n",
    "    for extractor in feature_extractors:\n",
    "        do_computations = True\n",
    "        if not overwrite and os.path.exists(results_dir):\n",
    "            existing_files = glob.glob(\"{}/{}/importance_per_residue.npy\".format(results_dir, extractor.name))\n",
    "            if len(existing_files) > 0 and not overwrite:\n",
    "                _log.debug(\"File %s already exists. skipping computations\", existing_files[0])\n",
    "                do_computations = False\n",
    "        if do_computations:\n",
    "            _log.info(\"Computing importance for extractor %s\", extractor.name)\n",
    "            extractor.extract_features()\n",
    "        p = extractor.postprocessing(working_dir=results_dir,\n",
    "                                     pdb_file=demystifying_dir + \"/all.pdb\",\n",
    "                                     feature_to_resids=feature_to_resids)\n",
    "        if do_computations:\n",
    "            p.average()\n",
    "            p.evaluate_performance()\n",
    "            p.persist()\n",
    "        else:\n",
    "            p.load()\n",
    "        postprocessors.append([p])\n",
    "    return np.array(postprocessors)\n",
    "\n",
    "    \n",
    "def _generate_structures(postprocessor):\n",
    "    cmd_template = \"bash single_render.sh {wdir} {view} {feature_type} {classifier_type} {traj_type} {state}\"\n",
    "    states = np.array([\"\"])\n",
    "    if postprocessor.extractor.supervised:\n",
    "        states = np.append(states, ligand_types if group_by_type else ligands)\n",
    "    for view in ['side', 'top']:\n",
    "        for state in states:\n",
    "            cmd = cmd_template.format(\n",
    "                wdir=demystifying_dir,\n",
    "                view=view,\n",
    "                feature_type=feature_type,\n",
    "                classifier_type=postprocessor.extractor.name,\n",
    "                traj_type=traj_type,\n",
    "                state= state + \"_grouped\" if group_by_type else state,\n",
    "            )\n",
    "            #_log.info(cmd)\n",
    "            try:\n",
    "                subprocess.run(cmd.strip().split(\" \"))\n",
    "            except Exception as err:\n",
    "                _log.exception(err)\n",
    "                _log.warning(\"Failed to execute command %s\", cmd)\n",
    "                \n",
    "\n",
    "def _generate_line_graphs(postprocessor):\n",
    "    p = postprocessor\n",
    "    states = np.array([None])\n",
    "    if postprocessor.supervised:\n",
    "        states = np.append(states, ligand_types if group_by_type else ligands)\n",
    "    #highlighted_residues = _get_default_important_residues(supervised=p.extractor.supervised)\n",
    "    importance_per_residue = p.importance_per_residue\n",
    "    for index, state in enumerate(states):\n",
    "        if state is None:\n",
    "            p.importance_per_residue = importance_per_residue\n",
    "        else:\n",
    "            p.importance_per_residue = p.importance_per_residue_and_cluster[:, index-1]\n",
    "        highlighted_residues = _get_important_residues(p.importance_per_residue, \n",
    "                                                       count_cutoff=10,\n",
    "                                                       importance_cutoff=0.2 if p.supervised else 0.1)\n",
    "        outfile =\"{outdir}/importance_per_residue_{traj_type}_{feature_type}_{classifier}.svg\".format(\n",
    "            outdir=p.get_output_dir(),\n",
    "            traj_type=traj_type,\n",
    "            feature_type=feature_type,\n",
    "            classifier=p.extractor.name + (\"\" if state is None else \"_\" + state),\n",
    "        ) \n",
    "        dm.visualization.visualize([[p]],\n",
    "                                show_importance=True,\n",
    "                                show_performance=False,\n",
    "                                show_projected_data=False,\n",
    "                                mixed_classes=False,\n",
    "                                plot_title=p.extractor.name + (\"\" if state is None else \" - \" + state),\n",
    "                                highlighted_residues=highlighted_residues,\n",
    "                                outfile=outfile)\n",
    "        display(SVG(filename=outfile))\n",
    "        plt.close()\n",
    "    p.importance_per_residue = importance_per_residue\n",
    "\n",
    "def _generate_snakeplots(postprocessor):\n",
    "    p = postprocessor\n",
    "    states = np.array([None])\n",
    "    if postprocessor.supervised:\n",
    "        states = np.append(states, ligand_types if group_by_type else ligands)\n",
    "    #TODO iterate over states\n",
    "    # see https://stackoverflow.com/questions/24726528/replacing-inner-contents-of-an-svg-in-python\n",
    "    from lxml import etree\n",
    "    SVGNS = u\"http://www.w3.org/2000/svg\"\n",
    "    with open(working_dir + \"/snake_adrb2_human.svg\", 'r') as file:\n",
    "        #Open the snakeplot downloaded from GPCRdb\n",
    "        template_svg = file.read()\n",
    "    \n",
    "    cmap = plt.get_cmap(\"Blues\")\n",
    "    \n",
    "    xml_data = etree.fromstring(template_svg)\n",
    "    for index, state in enumerate(states):\n",
    "        importances = p.importance_per_residue if state is None else p.importance_per_residue_and_cluster[:, index-1]\n",
    "        for r, imp in enumerate(importances):\n",
    "            resid = index_to_residue[r].resSeq\n",
    "            # We search for element 'text' with id='tile_text' in SVG namespace\n",
    "            ss = \"//{%s}circle[@id='%d']\" % (SVGNS, resid)\n",
    "            #print(ss)\n",
    "            find_residue = etree.ETXPath(ss)\n",
    "            # find_residue(xml_data) returns a list \n",
    "            # take the 1st element from the list, replace the fill   \n",
    "            #See https://docs.python.org/2/library/xml.etree.elementtree.html#modifying-an-xml-file\n",
    "            data = find_residue(xml_data)\n",
    "            if len(data) > 0:\n",
    "                color = mpl.colors.to_hex(cmap(imp))\n",
    "                #print(color)\n",
    "                data[0].set('fill', color)\n",
    "            else:\n",
    "                _log.warning(\"No SVG element found for residue %s\", resid)\n",
    "        #Save\n",
    "        outfile =\"{outdir}/snakeplot_importance_{traj_type}_{feature_type}_{classifier}.svg\".format(\n",
    "            outdir=p.get_output_dir(),\n",
    "            traj_type=traj_type,\n",
    "            feature_type=feature_type,\n",
    "            classifier=p.extractor.name + (\"\" if state is None else \"_\" + state),\n",
    "        )     \n",
    "        new_svg = etree.tostring(xml_data)\n",
    "        with open(outfile, \"wb\") as of:\n",
    "            of.write(new_svg)\n",
    "        display(SVG(filename=outfile))\n",
    "\n",
    "    \n",
    "\n",
    "def visualize_importance(postprocessors):\n",
    "    for [p] in postprocessors:\n",
    "        #_generate_line_graphs(p)\n",
    "        #_generate_snakeplots(p)\n",
    "        #_generate_structures(p)\n",
    "        continue\n",
    "        \n",
    "        \n",
    "kwargs = dict(\n",
    "    samples=samples.copy(),\n",
    "    labels=group_labels_by_type(labels) if group_by_type else labels.copy(),\n",
    "    label_names=ligand_types if group_by_type else ligands,\n",
    "    filter_by_distance_cutoff=False,\n",
    "    use_inverse_distances=True,\n",
    "    n_splits=1,\n",
    "    shuffle_datasets=False,\n",
    ")\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.668120Z",
     "start_time": "2020-06-23T13:54:09.160Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "supervised_feature_extractors = [\n",
    "    dm.feature_extraction.KLFeatureExtractor(n_iterations=3,**kwargs),\n",
    "    dm.feature_extraction.RandomForestFeatureExtractor(\n",
    "        n_iterations=10,\n",
    "        one_vs_rest=True,\n",
    "        classifier_kwargs=dict(n_estimators=100, n_jobs=-1),\n",
    "        **kwargs)\n",
    "]\n",
    "supervised_postprocessors = extract_features(supervised_feature_extractors, overwrite=False)\n",
    "visualize_importance(supervised_postprocessors)\n",
    "_log.debug(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.668632Z",
     "start_time": "2020-06-23T13:54:09.164Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#RBM requires data to be scaled with an upper limit of 1\n",
    "rbm_kwargs = dict(**kwargs)\n",
    "rbm_kwargs['samples'] = MinMaxScaler().fit_transform(scaler.inverse_transform(samples))\n",
    "rbm_kwargs['shuffle_datasets'] = True\n",
    "unsupervised_feature_extractors = [\n",
    "    dm.feature_extraction.PCAFeatureExtractor(classifier_kwargs=dict(n_components=2),\n",
    "                            supervised=True,\n",
    "                           variance_cutoff='2_components',\n",
    "                           **kwargs),\n",
    "   #dm.feature_extraction.RbmFeatureExtractor(\n",
    "    #    supervised=True,\n",
    "    #    n_iterations=50,\n",
    "    #    classifier_kwargs=dict(n_components=100, learning_rate=1e-3),\n",
    "    #    **rbm_kwargs \n",
    "    #)\n",
    "]\n",
    "unsupervised_postprocessors = extract_features(unsupervised_feature_extractors, \n",
    "                                               overwrite=False)\n",
    "visualize_importance(unsupervised_postprocessors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data with projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.669155Z",
     "start_time": "2020-06-23T13:54:09.170Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_importance_cmap(color=None, N = 1024):\n",
    "    #see https://matplotlib.org/3.1.0/tutorials/colors/colormap-manipulation.html\n",
    "    if color is None:\n",
    "        color =np.array([135, 21, 0])/256\n",
    "    elif isinstance(color, str):\n",
    "        color = mpl.colors.to_rgb(color)\n",
    "    vals = np.zeros((N, 4))\n",
    "    max_color = np.array([1,1,1])*0.95\n",
    "    min_color = color \n",
    "    vals[:, 0] = np.linspace(min_color[0], max_color[0], N)\n",
    "    vals[:, 1] = np.linspace(min_color[1], max_color[1], N)\n",
    "    vals[:, 2] = np.linspace(min_color[2], max_color[2], N)\n",
    "    return ListedColormap(vals)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Marks every snapshot with a ligand-specific marker\n",
    "colors every snapshots according to its index\n",
    "\"\"\"\n",
    "def plot_activation_ligands(X, ligands=ligands, labels=labels, \n",
    "                 xlabel=None, ylabel=None, show_title=True,\n",
    "                 method=None, savefig=True, alpha=1., subplots=False, ncols=3):\n",
    "\n",
    "    #values = ligand_labels if group_by_type else ligands\n",
    "    values = ligands\n",
    "    if subplots:\n",
    "        fig, axs = plt.subplots(ncols=ncols, \n",
    "                        nrows=1+int(len(ligands)/ncols), \n",
    "                        figsize=(12, 10),\n",
    "                        squeeze=True,\n",
    "                        sharey=True, \n",
    "                        sharex=True)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "    row, col = 0, 0\n",
    "    cmap = create_importance_cmap()   \n",
    "    for i, lig in enumerate(values):\n",
    "        l = _fix_label(lig)\n",
    "        indices = labels[:, i] == 1\n",
    "        if subplots:\n",
    "            plt.sca(axs[row, col])\n",
    "        if len(X.shape) < 2 or X.shape[1] == 1:\n",
    "            plt.hist(X[indices], label=l, alpha=alpha, color=colors[i], density=True)\n",
    "        else:\n",
    "            xx = X[indices]\n",
    "            plt.scatter(xx[:, 0], xx[:, 1], \n",
    "                        label=l, \n",
    "                        alpha=alpha, \n",
    "                        marker=markers.get(lig ,\".\"),\n",
    "                        #edgecolors=cmap(1),                        \n",
    "                        color=cmap(np.linspace(0, 1, len(xx))), \n",
    "                        s=8)\n",
    "            #plt.scatter(xx[0,0 ], xx[0, 1], marker='d', color='black')\n",
    "            #plt.scatter(xx[-1,0 ], xx[-1, 1], marker='^', color='black')\n",
    "        col += 1\n",
    "        if col >= ncols:\n",
    "            col = 0\n",
    "            row += 1        \n",
    "        plt.legend()\n",
    "    if show_title:\n",
    "        plt.title(\"{}\\n{}\".format(\"\" if method is None else method, traj_type))\n",
    "    if xlabel is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel is not None:\n",
    "        plt.ylabel(ylabel)    \n",
    "    if savefig:\n",
    "        plt.savefig(\"output/projections/{}_{}_{}.svg\".format(method, feature_type, traj_type))\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\"\n",
    "Marks every snapshot with a ligand-specific color\n",
    "\"\"\"\n",
    "def plot_state_ligands(X, ligands=ligands, labels=labels, \n",
    "                 xlabel=None, ylabel=None, show_title=True,\n",
    "                 method=None, savefig=True, alpha=1., subplots=False, ncols=3):\n",
    "    #values = ligand_labels if group_by_type else ligands\n",
    "    values = ligands\n",
    "    if subplots:\n",
    "        fig, axs = plt.subplots(ncols=ncols, \n",
    "                        nrows=1+int(len(ligands)/ncols), \n",
    "                        figsize=(12, 10),\n",
    "                        squeeze=True,\n",
    "                        sharey=True, \n",
    "                        sharex=True)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "    row, col = 0, 0\n",
    "    for i, lig in enumerate(values):\n",
    "        l = _fix_label(lig)\n",
    "        indices = labels[:, i] == 1\n",
    "        if subplots:\n",
    "            plt.sca(axs[row, col])\n",
    "        if len(X.shape) < 2 or X.shape[1] == 1:\n",
    "            plt.hist(X[indices], label=l, alpha=alpha, color=colors[i], density=True)\n",
    "        else:\n",
    "            plt.scatter(X[indices, 0], X[indices, 1], \n",
    "                        label=l, \n",
    "                        alpha=alpha, \n",
    "                        color=colors[i], \n",
    "                        marker=markers.get(lig ,\".\"),    \n",
    "                        s=2)\n",
    "        col += 1\n",
    "        if col >= ncols:\n",
    "            col = 0\n",
    "            row += 1        \n",
    "        plt.legend()\n",
    "    if show_title:\n",
    "        plt.title(\"{}\\n{}\".format(\"\" if method is None else method, traj_type))\n",
    "    if xlabel is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel is not None:\n",
    "        plt.ylabel(ylabel)    \n",
    "    if savefig:\n",
    "        plt.savefig(\"output/projections/{}_{}_{}.svg\".format(method, feature_type, traj_type))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_ligands(X, **kwargs):\n",
    "    if traj_type == 'strings':\n",
    "        return plot_activation_ligands(X, **kwargs)\n",
    "    else:\n",
    "        return plot_state_ligands(X, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.670083Z",
     "start_time": "2020-06-23T13:54:09.179Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()#n_components=2)\n",
    "X_pca = pca.fit_transform(samples)\n",
    "plot_ligands(X_pca[:, :2], method=\"PCA\")\n",
    "plot_ligands(X_pca[:, 2:4], method=\"PCA2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.670541Z",
     "start_time": "2020-06-23T13:54:09.183Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import *\n",
    "# Setting parameters to make the method more deterministic, \n",
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "tsne_config=dict(n_components=2, random_state=0, n_jobs=-1,perplexity=30, learning_rate=200)\n",
    "_log.debug(\"Using config %s\", tsne_config)\n",
    "tsne = TSNE(**tsne_config)\n",
    "X_tsne = tsne.fit_transform(samples)\n",
    "plot_ligands(X_tsne, method=\"TSNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS\n",
    "see https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.670995Z",
     "start_time": "2020-06-23T13:54:09.194Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import *\n",
    "\n",
    "config=dict(n_components=2, \n",
    "            random_state=0, \n",
    "            metric=True,\n",
    "            n_jobs=-1)\n",
    "_log.debug(\"Using config %s\", config)\n",
    "mds = MDS(**config)\n",
    "X_mds = mds.fit_transform(samples)\n",
    "plot_ligands(X_mds, method=\"MDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demystiyfing results\n",
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.671463Z",
     "start_time": "2020-06-23T13:54:09.203Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_id(cv):\n",
    "    d = cv.name\n",
    "    d = d.replace(\"^-1(closest-heavy)\", \"\")\n",
    "    d = d.replace(\"|\", \"\")\n",
    "    return d\n",
    "\n",
    "def project_top_features(postprocessors, max_features=6, distance_cutoff=0.5):\n",
    "    if 'inv' in feature_type:\n",
    "        distances = 1/scaler.inverse_transform(samples)\n",
    "    else:\n",
    "        distance = scaler.inverse_transform(samples)\n",
    "    \n",
    "    for [p] in postprocessors:\n",
    "        imps = p.get_important_features()\n",
    "        counter = 0\n",
    "        for i, (idx1, imp1) in enumerate(imps):\n",
    "            if i % 2 == 1:\n",
    "                continue\n",
    "            idx2, imp2 = imps[i+1]\n",
    "            idx1, idx2 = int(idx1), int(idx2)\n",
    "            id1, id2 = clean_id(cvs[idx1]), clean_id(cvs[idx2])\n",
    "            X_imps = distances[:, [idx1, idx2]]\n",
    "            if X_imps.min() > distance_cutoff:\n",
    "                continue\n",
    "            _log.debug(\"#%s with importance %s and %s: %s-%s\", i, imp1, imp2, id1, id2)\n",
    "            method=\"{}_{}_{}\".format(p.extractor.name, id1, id2)\n",
    "            _log.debug(method)\n",
    "            plot_ligands(X_imps, \n",
    "                         xlabel=id1,\n",
    "                         ylabel=id2,\n",
    "                         method=method)\n",
    "            compute_pairwise_similarity(X_imps, \n",
    "                                method=neg_frame_to_frame_distance, \n",
    "                                title=method) \n",
    "            counter += 1\n",
    "            if counter >= max_features:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.671926Z",
     "start_time": "2020-06-23T13:54:09.207Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "project_top_features(supervised_postprocessors)\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically find top ranked features for certain residues with close contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.672401Z",
     "start_time": "2020-06-23T13:54:09.218Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_close_contacts(postprocessors, limit = 10, distance_cutoff=0.6,\n",
    "                        importance_cutoff=0.5,\n",
    "                        states=None,\n",
    "                       to_find = [321, 206, 207, 327, 312, 315],\n",
    "                    to_ignore=[]\n",
    "                       ):\n",
    "    if 'inv' in feature_type:\n",
    "        distances = 1/scaler.inverse_transform(samples)\n",
    "    else:\n",
    "        distance = scaler.inverse_transform(samples)\n",
    "    to_find = [str(s) for s in to_find]\n",
    "    to_ignore=[str(s) for s in to_ignore]\n",
    "    for [p] in postprocessors:\n",
    "        counter = 0\n",
    "        imps = p.get_important_features(states=states)\n",
    "        for i in range(0,len(imps),2):\n",
    "\n",
    "            idx1, imp1 = imps[i]\n",
    "            idx2, imp2 = imps[i+1]\n",
    "            if imp1 < importance_cutoff:\n",
    "                break         \n",
    "            idx1, idx2 = int(idx1), int(idx2)\n",
    "            id1, id2 = clean_id(cvs[idx1]), clean_id(cvs[idx2])\n",
    "            method=\"{}_{}_{}\".format(p.extractor.name, id1, id2)\n",
    "            found = False\n",
    "            for ti in to_ignore:\n",
    "                if ti in method:\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                continue\n",
    "            for tf in to_find:\n",
    "                if tf in method:\n",
    "                    found = True\n",
    "            if not found:\n",
    "                continue\n",
    "            X_imps = distances[:, [idx1, idx2]]\n",
    "            if X_imps.min() > distance_cutoff:\n",
    "                #Look for close contacts\n",
    "                continue\n",
    "            _log.debug(\"#%s with importance %s and %s: %s-%s. States: %s\", i, imp1, imp2, id1, id2, states)\n",
    "            plot_ligands(X_imps, \n",
    "                         xlabel=id1, ylabel=id2,\n",
    "                         method=method)\n",
    "            counter += 1\n",
    "            if counter == limit:\n",
    "                break\n",
    "\n",
    "                \n",
    "if group_by_type:\n",
    "    find_close_contacts(supervised_postprocessors)\n",
    "else:\n",
    "    for l_idx, ligand in enumerate(ligands):\n",
    "        _log.info(\"---------%s-------\", ligand)\n",
    "        find_close_contacts(supervised_postprocessors, states=[l_idx], \n",
    "                            limit=3, \n",
    "                            distance_cutoff=0.5, \n",
    "                            importance_cutoff=0.1)\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.672847Z",
     "start_time": "2020-06-23T13:54:09.222Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "project_top_features(unsupervised_postprocessors)\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select residues to plot against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.673496Z",
     "start_time": "2020-06-23T13:54:09.226Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_feature_index(residues, feature_to_resids=feature_to_resids):\n",
    "    for idx, row in enumerate(feature_to_resids):\n",
    "        if len(row) == len(residues):\n",
    "            found = True\n",
    "            for r in row:\n",
    "                if r not in residues:\n",
    "                    found = False\n",
    "                    break\n",
    "            if found:\n",
    "                return idx\n",
    "    _log.warning(\"No features found for residues %s\", residues)\n",
    "            \n",
    "def find_for_residues(postprocessor, \n",
    "                       residue_pairs = [],\n",
    "                      ligands=ligands,\n",
    "                       ):\n",
    "    p = postprocessor\n",
    "    if 'inv' in feature_type:\n",
    "        distances = 1/scaler.inverse_transform(samples)\n",
    "    else:\n",
    "        distance = scaler.inverse_transform(samples)\n",
    "    imps = p.feature_importances.mean(axis=1)\n",
    "    imps = (imps-imps.min())/(imps.max()-imps.min())\n",
    "    for pair1, pair2 in residue_pairs:\n",
    "        idx1 = find_feature_index(pair1)\n",
    "        idx2  = find_feature_index(pair2)\n",
    "        cv1 = cvs[idx1]\n",
    "        cv2 = cvs[idx2]\n",
    "        imp1 = imps[idx1]\n",
    "        imp2 = imps[idx2]\n",
    "        idx1, idx2 = int(idx1), int(idx2)\n",
    "        id1, id2 = clean_id(cv1), clean_id(cv2)\n",
    "        method=\"{}_{}_{}\".format(p.extractor.name, id1, id2)\n",
    "        X_imps = distances[:, [idx1, idx2]]\n",
    "        _log.debug(\"with importance %s and %s: %s-%s\", imp1, imp2, id1, id2)\n",
    "        plot_ligands(X_imps, \n",
    "                     ligands=ligands,\n",
    "                     show_title=False,\n",
    "                     xlabel=id1, ylabel=id2,\n",
    "                     method=method)\n",
    "\n",
    "#Pathways\n",
    "find_for_residues(\n",
    "    supervised_postprocessors[0, 0],\n",
    "    [\n",
    "        [(118, 206), (284, 321)],\n",
    "        [(277, 327), (281, 325)],\n",
    "        [(207, 307), (203, 338)],\n",
    "        [(131, 272), (326, 285)], #Include if we need a scatter plot with C285\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ligand specific - primarily single states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.673968Z",
     "start_time": "2020-06-23T13:54:09.230Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Singling out salmeterol and alprenolol ligands\n",
    "ligand_projection_mapping =  dict(\n",
    "    apo=[(222, 271), (225, 268), (272, 131), (75, 322), (127, 321)],\n",
    "    carazolol=[(51, 319)],\n",
    "    alprenolol=[(79, 321), (51, 319)],\n",
    "    timolol=[(274, 321), (50, 327)],\n",
    "    salmeterol=[(285, 326), (136, 272), (321, 326)],\n",
    "    #For adrenaline and p0g, see also agonist vs non-agonist plots above\n",
    "    adrenaline=[(113, 308),(275, 326)],\n",
    "    p0g=[(275, 326)]\n",
    ")\n",
    "find_for_residues(\n",
    "    supervised_postprocessors[0, 0],\n",
    "[\n",
    "    [(225, 268), (272, 131)], #apo\n",
    "   # [(79, 321), (51, 319)], #carzolol and alprenolol\n",
    "    #[(127, 321), (51, 319)], #agonists, apo, alprenolol and carazolol\n",
    "    [(79, 321), (79, 322)], #apo and alprenolol     \n",
    "    [(274, 321), (50, 327)], #timolol\n",
    "    [(321, 326), (136, 272)], #salmeterol\n",
    "    [(51, 319), (275, 326)], #apo, alprenolol and carazolol, adrenaline and p0g\n",
    "]    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare similarity \n",
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.674471Z",
     "start_time": "2020-06-23T13:54:09.235Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "eps = 1e-4 \n",
    "\n",
    "def jaccard_similarity(x1, x2):\n",
    "    #Probably not a good measure after all!\n",
    "    #from https://stackoverflow.com/questions/46975929/how-can-i-calculate-the-jaccard-similarity-of-two-lists-containing-strings-in-py\n",
    "    intersection = len(list(set(x1).intersection(x2)))\n",
    "    union = (len(x1) + len(x2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def cluster_similarity(x1, x2):\n",
    "    if len(x1) != len(x2):\n",
    "        raise Exception(\"Clusters must be of same size\")\n",
    "    x1,x2 = x1.squeeze(), x2.squeeze()\n",
    "    clusters = set(x1)\n",
    "    for xx in x2:\n",
    "        clusters.add(xx)\n",
    "    similarity = 0\n",
    "    diff = 0\n",
    "    for c in clusters:\n",
    "        x1c = x1[x1 == c]\n",
    "        x2c = x2[x2 == c]\n",
    "        diff += abs(len(x1c)-len(x2c))        \n",
    "    similarity = 1 / (1 + diff)\n",
    "    return similarity\n",
    "        \n",
    "def inv_center_dist(x1, x2):\n",
    "    return 1/(1+np.linalg.norm(x1.mean(axis=0) - x2.mean(axis=0)))\n",
    "\n",
    "def KL_divergence(x1, x2, bin_width=None, symmetric=False):\n",
    "    \"\"\"\n",
    "    Compute Kullback-Leibler divergence\n",
    "    From demystifying repo\n",
    "    \"\"\"\n",
    "    n_features = x1.shape[1] \n",
    "\n",
    "    DKL = np.zeros(n_features)\n",
    "    if bin_width is not None:\n",
    "        tmp_bin_width = bin_width\n",
    "\n",
    "    for i_feature in range(n_features):\n",
    "        xy = np.concatenate((x1[:, i_feature], x2[:, i_feature]))\n",
    "        bin_min = np.min(xy)\n",
    "        bin_max = np.max(xy)\n",
    "\n",
    "        if bin_width is None:\n",
    "            tmp_bin_width = np.std(x1[:, i_feature])\n",
    "            if tmp_bin_width == 0:\n",
    "                tmp_bin_width = 0.1  # Set arbitrary bin width if zero\n",
    "        else:\n",
    "            tmp_bin_width = self.bin_width\n",
    "\n",
    "        if tmp_bin_width >= (bin_max - bin_min):\n",
    "            DKL[i_feature] = 0\n",
    "        else:\n",
    "            bin_n = int((bin_max - bin_min) / tmp_bin_width)\n",
    "            x1_prob = np.histogram(x1[:, i_feature], bins=bin_n, range=(bin_min, bin_max), density=True)[0] + 1e-9\n",
    "            x2_prob = np.histogram(x2[:, i_feature], bins=bin_n, range=(bin_min, bin_max), density=True)[0] + 1e-9\n",
    "            #TODO should we use symmetrized KL as done below?\n",
    "            if symmetric:\n",
    "                DKL[i_feature] = 0.5 * (entropy(x1_prob, x2_prob) + entropy(x2_prob, x1_prob))\n",
    "            else:\n",
    "                DKL[i_feature] = entropy(x1_prob, x2_prob)\n",
    "    return DKL\n",
    "\n",
    "def avg_KL(x1,x2):\n",
    "    return KL_divergence(x1,x2).mean()\n",
    "\n",
    "def neg_avg_KL(x1,x2):\n",
    "    return -avg_KL(x1,x2)\n",
    "\n",
    "\n",
    "def frame_to_frame_distance(x1, x2):\n",
    "    dist = 0\n",
    "    for xx1 in x1:\n",
    "        dist += np.linalg.norm(xx1-x2, axis=1).sum()\n",
    "        #for xx2 in x2:\n",
    "        #    dist += np.linalg.norm(xx1-xx2)\n",
    "    dist /= x1.shape[0]*x2.shape[0]  \n",
    "    return dist\n",
    "        \n",
    "def compute_pairwise_similarity(X, method, title=None, ligand_labels=ligand_labels,\n",
    "                                ligands=ligands, ligand_types=ligand_types):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X[:,np.newaxis]\n",
    "    ligands = ligand_types if group_by_type else ligands\n",
    "    data = np.zeros((len(ligands), (len(ligands)))) + np.nan\n",
    "    for idx1, l1 in enumerate(ligands):\n",
    "        #Partition data\n",
    "        l1_indices = ligand_labels[:,idx1] == 1\n",
    "        x1 = X[l1_indices]\n",
    "        for idx2, l2 in enumerate(ligands):\n",
    "            l2_indices = ligand_labels[:, idx2] == 1\n",
    "            x2 = X[l2_indices]\n",
    "            #print(l1,l2, x1.shape, x2.shape)\n",
    "            data[idx1, idx2 ] = method(x1, x2)        \n",
    "        # normalize\n",
    "        #stats[:] = (stats - stats.min())/(stats.max() - stats.min())\n",
    "    # normalize\n",
    "    data = (data - data.min())/(data.max() - data.min())\n",
    "    \n",
    "    #Plot, see https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "    fig, ax = plt.subplots()\n",
    "    im = plt.imshow(data, cmap=plt.get_cmap(\"Blues\")) #YlGnBu\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(ligands)))\n",
    "    ax.set_yticks(np.arange(len(ligands)))\n",
    "    # ... and label them with the respective list entries    \n",
    "    ax.set_xticklabels(ligands)\n",
    "    ax.set_yticklabels(ligands)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"Similarity\", rotation=-90, va=\"bottom\")\n",
    "    # Show and save\n",
    "    plt.title(\"{}\\n{}\".format(title, traj_type))\n",
    "    plt.tight_layout(pad=0.3)\n",
    "    plt.savefig(\"output/similarities/{}_feature_type_{}_{}{}.svg\".format(title, feature_type, \n",
    "                                                                         traj_type,\n",
    "                                                                         \"_bytype\" if group_by_type else \"\"\n",
    "                                                                        ))\n",
    "    plt.show()        \n",
    "\n",
    "def inv_frame_to_frame_distance(x1, x2):\n",
    "    return 1/(eps + frame_to_frame_distance(x1,x2))\n",
    "\n",
    "def neg_frame_to_frame_distance(x1, x2):\n",
    "    return -frame_to_frame_distance(x1,x2)\n",
    "\n",
    "def inv_KL(x1, x2):\n",
    "    kl = KL_divergence(x1,x2)\n",
    "    return 1/(eps+ kl )\n",
    "\n",
    "def neg_KL(x1, x2):\n",
    "    return -KL_divergence(x1,x2)\n",
    "\n",
    "KL = KL_divergence\n",
    "\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Eucledian distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T13:54:52.674965Z",
     "start_time": "2020-06-23T13:54:09.239Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compute_pairwise_similarity(samples, \n",
    "                            method=neg_frame_to_frame_distance, \n",
    "                            title=\"full-frame_to_frame_distance\") \n",
    "_log.info(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
